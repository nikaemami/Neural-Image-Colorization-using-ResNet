{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a200f901-0f7f-4e09-9bc0-a37686a49dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/ne2213/projects/tmp_packages')\n",
    "sys.path.append('/scratch/ne2213/projects/tmp_packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097d06bb-0a11-42ba-977a-66f45b85e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pynwb import NWBHDF5IO\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "import h5py\n",
    "import tempfile\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Ignoring cached namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7eca38-bc96-465b-8f74-40145dc8b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DANDI IDs and output directory\n",
    "dandi_ids = {\n",
    "    \"EY\": \"000541\",\n",
    "    \"SK1\": \"000565\",\n",
    "    \"NP\": \"000715\",\n",
    "    \"SK2\": \"000472\",\n",
    "    \"HL\": \"000714\",\n",
    "    \"KK\": \"000692\",\n",
    "    \"SF\": \"000776\"\n",
    "}\n",
    "\n",
    "# Define the base output directory\n",
    "base_output_dir = \"/scratch/ne2213/projects/CV-Project/NWBelegans/extracted_data_all/\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed28217-df16-432c-90f4-e70522ca2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each dataset\n",
    "for label, dandi_id in dandi_ids.items():\n",
    "    print(f\"Processing dataset {label} with DANDI ID {dandi_id}...\")\n",
    "\n",
    "    # Initialize lists to store NeuroPAL data\n",
    "    neuron_voxel_masks = []\n",
    "    neuron_labels = []\n",
    "    grid_spacing = []\n",
    "    rgb_values = []  # Store RGB values for each voxel\n",
    "    structured_data = []  # Store full structured data in a DataFrame\n",
    "\n",
    "    with DandiAPIClient() as client:\n",
    "        print(f\"Connecting to DANDI API for {label}...\")\n",
    "        \n",
    "        # Get the dandiset\n",
    "        dandiset = client.get_dandiset(dandi_id, 'draft')\n",
    "        print(f\"Retrieved dandiset {dandi_id} for {label}.\")\n",
    "\n",
    "        # Iterate through the assets in the dandiset\n",
    "        for asset in dandiset.get_assets():\n",
    "            print(f\"Processing asset: {asset.identifier}...\")\n",
    "\n",
    "            # Get the asset URL for downloading\n",
    "            s3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "            # Create a temporary file to store the downloaded asset\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                temp_file_path = temp_file.name\n",
    "                print(f\"Downloading asset {asset.identifier} to temporary file...\")\n",
    "                \n",
    "                # Download the asset using requests\n",
    "                response = requests.get(s3_url, stream=True)\n",
    "                if response.status_code == 200:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        temp_file.write(chunk)\n",
    "                    print(f\"Download complete for {asset.identifier}.\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {s3_url}. Status code: {response.status_code}\")\n",
    "                    continue\n",
    "\n",
    "            # Open the NWB file and extract data\n",
    "            try:\n",
    "                print(f\"Opening NWB file: {temp_file_path}...\")\n",
    "                with h5py.File(temp_file_path, 'r') as f:\n",
    "                    with NWBHDF5IO(file=f, mode='r', load_namespaces=True) as io:\n",
    "                        read_nwb = io.read()\n",
    "                        print(f\"Successfully read NWB file for {asset.identifier}.\")\n",
    "\n",
    "                        # Extract NeuroPAL data\n",
    "                        try:\n",
    "                            seg = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons'].voxel_mask[:]\n",
    "                            labels = read_nwb.processing['NeuroPAL']['NeuroPALSegmentation']['NeuroPALNeurons']['ID_labels'][:]\n",
    "                            channels = read_nwb.acquisition['NeuroPALImageRaw'].RGBW_channels[:]\n",
    "                            image = read_nwb.acquisition['NeuroPALImageRaw'].data[:]\n",
    "                            scale = read_nwb.imaging_planes['NeuroPALImVol'].grid_spacing[:]\n",
    "\n",
    "                            # Process labels into strings\n",
    "                            labels = [\"\".join(label) for label in labels]\n",
    "                            print(f\"Extracted NeuroPAL data for {asset.identifier}.\")\n",
    "\n",
    "                            # Create a structured DataFrame for the voxel data\n",
    "                            blobs = pd.DataFrame.from_records(seg, columns=['x', 'y', 'z', 'weight'])\n",
    "                            blobs = blobs[\n",
    "                                (blobs['x'] < image.shape[0]) &\n",
    "                                (blobs['y'] < image.shape[1]) &\n",
    "                                (blobs['z'] < image.shape[2])\n",
    "                            ]\n",
    "\n",
    "                            # Add RGB values\n",
    "                            blobs[['R', 'G', 'B']] = [\n",
    "                                image[int(row['x']), int(row['y']), int(row['z']), channels[:3]]\n",
    "                                for _, row in blobs.iterrows()\n",
    "                            ]\n",
    "\n",
    "                            # Add scaled spatial positions\n",
    "                            blobs[['xr', 'yr', 'zr']] = [\n",
    "                                [row['x'] * scale[0], row['y'] * scale[1], row['z'] * scale[2]]\n",
    "                                for _, row in blobs.iterrows()\n",
    "                            ]\n",
    "\n",
    "                            # Add labels to the blobs\n",
    "                            blobs['ID'] = labels[:len(blobs)]  # Align the labels with blobs\n",
    "\n",
    "                            # Append to lists for saving\n",
    "                            neuron_voxel_masks.append(seg)\n",
    "                            neuron_labels.append(labels)\n",
    "                            grid_spacing.append(scale)\n",
    "                            rgb_values.append(blobs[['R', 'G', 'B']].values.tolist())\n",
    "                            structured_data.append(blobs)\n",
    "\n",
    "                            print(f\"Processed structured data for {asset.identifier}.\")\n",
    "                        except KeyError as e:\n",
    "                            print(f\"KeyError while processing {asset.identifier}: {e}. Skipping this asset.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {temp_file_path}: {e}\")\n",
    "\n",
    "            # Remove the temporary file\n",
    "            os.remove(temp_file_path)\n",
    "            print(f\"Temporary file {temp_file_path} removed.\")\n",
    "\n",
    "    # Combine all data into a dictionary\n",
    "    print(f\"Combining all data for {label}...\")\n",
    "    data = {\n",
    "        \"neuron_voxel_masks\": neuron_voxel_masks,\n",
    "        \"neuron_labels\": neuron_labels,\n",
    "        \"grid_spacing\": grid_spacing,\n",
    "        \"rgb_values\": rgb_values,\n",
    "        \"structured_data\": pd.concat(structured_data, ignore_index=True) if structured_data else None\n",
    "    }\n",
    "\n",
    "    # Save all data into a single .pkl file\n",
    "    output_file = os.path.join(base_output_dir, f\"{label}_data.pkl\")\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Finished saving all data for {label} in {output_file}.\\n\")\n",
    "\n",
    "print(\"All datasets processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
